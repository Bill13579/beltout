import torch
import torch.nn as nn
from .transformer import MultiHeadedSelfAttentionBlock, FeedForward

class PitchMvmtEncoder(nn.Module):
    """
    To understand the problem of why the original model seems to struggle immensely with pitch, we must first look closely at its operations.

    The very heart of this whole model is the `CausalConditionalCFM`. This is a flow-matching model, learning the average vector field of velocities that best moves a random point away from the noise distribution towards where the real data distribution is more present.

    The original model would do this flow-matching process while also taking in various conditioning signals. The full input parameters into the original flow-matching model is as follows:

    | Vector Component | Source | Dimensions | Role / Conditioning Provided |
    |------------------|--------|------------|------------------------------|
    | Noisy Mel `x` | Conditional Flow-Matching Process | (80, T) | "What am I currently looking at? (The state to be refined)" |
    | Prosodic and phonetic context `mu` | *From user's performance.* `s3_tokens`, through an adapter `encoder_proj` projecting it from 512-dims to 80-dims | (80, T) | "What should the content and rhythm be?" |
    | Timbre `spks` | `x-vector`, through an adapter `spk_embed_affine_layer` projecting it from 192-dims to 80-dims | (80, 1) repeated to (80, T) | "What's the target timbre?" |
    | Prompt `cond=prompt_feat` | *From target's performance.* The original target's performance converted into a MEL spectrogram format. | (80, ?), with the rest filled in with zeroes if not long enough | "Here's a concrete example of the target voice." |
    | Total |  | (320, T) |  |

    However, testing reveals that the last signal ended up being quite weak in the final model. Instead, we can replace it as follows:

    | Vector Component | Source | Dimensions | Role / Conditioning Provided |
    |------------------|--------|------------|------------------------------|
    | Noisy Mel `x` | Conditional Flow-Matching Process | (80, T) | "What am I currently looking at? (The state to be refined)" |
    | Prosodic and phonetic context `mu` | *From user's performance.* `s3_tokens`, through an adapter `encoder_proj` projecting it from 512-dims to 80-dims | (80, T) | "What should the content and rhythm be?" |
    | Timbre `spks` | `x-vector`, through an adapter `spk_embed_affine_layer` projecting it from 192-dims to 80-dims | (80, 1) repeated to (80, T) | "What's the target timbre?" |
    | ~~Prompt `cond=prompt_feat`~~<br><br>**Pitch context** | ~~*From target's performance.* The original target's performance converted into a MEL spectrogram format.~~<br><br>***From user's performance.* Some sort of embedding of how the pitch is changing in the user's performance.** | ~~(80, ?), with the rest filled in with zeroes if not long enough~~<br><br>**(80, T)** | ~~"Here's a concrete example of the target voice."~~<br><br>**"What should the pitch be?"** |
    | Total |  | (320, T) |  |

    The last row, the pitch context, is what is learned and generated by this model. In many ways, this model does for pitch what the s3 tokenizer does for content.
    The technical report goes into further detail about some of the design decisions made with the model.
    """
    def __init__(self, n_crepe_frame_in_mel_frame=2, tr_fine={ "n_heads": 16, "d_k": 4, "d_v": 4 }, tr_coarse={ "n_heads": 16, "d_k": 8, "d_v": 8 }, crepe_embedding_size=256, coarse_factor=8):
        super().__init__()
        self.n_crepe_frame_in_mel_frame = n_crepe_frame_in_mel_frame

        # Step 1.1: Fine-grained harmonic attention
        # self.attention_freq = MultiHeadedSelfAttentionBlock(d_model=3, n_heads=tr_fine["n_heads"], d_k=tr_fine["d_k"], d_v=tr_fine["d_v"], dropout=0.1)
        # self.ff_freq = FeedForward(d_model=3, d_ff=3 * 2)
        # self.norm_freq = nn.LayerNorm(3)

        # Step 2: Coarse-grained abstract attention
        coarse_group_size = 3*coarse_factor
        self.attention_coarse = MultiHeadedSelfAttentionBlock(d_model=coarse_group_size, n_heads=tr_coarse["n_heads"], d_k=tr_coarse["d_k"], d_v=tr_coarse["d_v"], dropout=0.1)
        self.ff_coarse = FeedForward(d_model=coarse_group_size, d_ff=coarse_group_size * 2)
        self.norm_coarse = nn.LayerNorm(coarse_group_size)

        # Step 3: Final projection
        self.final_norm = nn.LayerNorm(coarse_group_size)
        self.final_proj = nn.Linear((self.n_crepe_frame_in_mel_frame * (crepe_embedding_size // coarse_factor)) * coarse_group_size, 80)

        self.coarse_group_size = coarse_group_size

    def forward(self, crepe_embedding):
        # Input crepe_embedding shape: (B, 2, 256) (or (B, n_crepe_frame_in_mel_frame, crepe_embedding_size))
        B, T, D = crepe_embedding.shape

        # 1. Prepare unified sequence with positional encodings
        # Reshape to (B, (D*2), 1) (or (B, self.n_crepe_frame_in_mel_frame * D, 1))
        unified_seq = crepe_embedding.reshape(B, T * D, 1)

        # Create positional encodings
        freq_pos = torch.linspace(-1.0, 1.0, T * D, device=crepe_embedding.device).view(1, -1, 1).expand(B, -1, -1)
        # We use torch.linspace to create n_crepe evenly spaced points from -1.0 to 1.0.
        # For T=2, this gives [-1.0, 1.0]
        # For T=4, this gives [-1.0, -0.333, 0.333, 1.0]
        base_time_steps = torch.linspace(-1.0, 1.0, T, device=crepe_embedding.device)
        # `torch.repeat_interleave` will take each element from `base_time_steps`
        # and repeat it `D` times consecutively.
        # e.g., for n_crepe=2, it produces: [-1, -1, ..., -1 (D times), 1, 1, ..., 1 (D times)]
        time_pos_flat = torch.repeat_interleave(base_time_steps, repeats=D)
        # The final shape must be (B, n_crepe * D, 1) to match the other tensors.
        time_pos = time_pos_flat.view(1, -1, 1).expand(B, -1, -1)
        
        # Final input shape: (B, (D*2), 3) (or (B, self.n_crepe_frame_in_mel_frame * D, 3))
        x = torch.cat([unified_seq, freq_pos, time_pos], dim=-1)

        # 1.1. Fine-grained harmonic attention
        # No need for risiduals, since the attention block includes it.
        # x, _ = self.attention_freq(x)
        # x = self.norm_freq(self.ff_freq(x))

        # 2. Reshape for coarse attention
        x = x.view(B, -1, self.coarse_group_size) # (B, self.n_crepe_frame_in_mel_frame * (D // coarse_factor), self.coarse_group_size)

        # 2.1. Coarse-grained abstract attention
        x, _ = self.attention_coarse(x)
        x = self.norm_coarse(self.ff_coarse(x))
        
        # 3. Final projection
        x = self.final_norm(x)
        x = x.flatten(1) # Shape: (B, (self.n_crepe_frame_in_mel_frame * (crepe_embedding_size // coarse_factor)) * coarse_group_size)
        x = self.final_proj(x) # Shape: (B, 80)

        return x

